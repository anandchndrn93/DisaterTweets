{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9R8ZWXibk20",
    "outputId": "b0976f5b-9327-4a16-e003-bc7363c15488"
   },
   "outputs": [],
   "source": [
    "!pip install textaugment\n",
    "!pip3 install geograpy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scd97AhTcVAC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt \n",
    "import geograpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vHgtJEAbk3C"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from textaugment import EDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pickle\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import gensim\n",
    "import pickle\n",
    "DATAPATH = \"/content//\"\n",
    "TEMP_DF = \"TEMP_DF.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLciTiq2bk3D"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZw4AMzzbk3E"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"/content/train.csv\") #Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-sVcGCPibk3E",
    "outputId": "8996fdf0-cad2-4036-ea4c-6445ea1d18d8"
   },
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGDcQaMLbk3G",
    "outputId": "b28a8d38-4d28-4140-9461-c6700deda31e"
   },
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jjLlSOJbk3G",
    "outputId": "4528fd68-70a6-4baf-82f3-5a4d2ce1bcaa"
   },
   "outputs": [],
   "source": [
    "data_train.isnull().sum() #Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZ4-0Dnybk3H"
   },
   "outputs": [],
   "source": [
    "\n",
    "max_words = 4000\n",
    "tokenizer_file = 'tokenizer.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtGSSurybk3H",
    "outputId": "fb0b5ef6-af4a-4b96-8a2b-f94d455a79d7"
   },
   "outputs": [],
   "source": [
    "#import and create an EDA example\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from textaugment import EDA\n",
    "t = EDA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CA0PVZFRbk3I",
    "outputId": "d128686b-cc27-4c2b-bbca-7172709e2a24"
   },
   "outputs": [],
   "source": [
    "#check for a place in each tweet and populate the location.\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "location_list=data_train[\"location\"].unique() #list of all the locations in the location feature\n",
    "def fill_location(sentence):\n",
    "    words=sentence.split()\n",
    "    location = [ loc for loc in location_list if loc in words ] #check if the tweet contains a location already present in location feature\n",
    "    try:\n",
    "        location=location[0]\n",
    "    except:\n",
    "       # location=np.nan\n",
    "        places = geograpy.get_place_context(text=sentence) #check for any location in the tweet            \n",
    "        countries =places.countries\n",
    "        cities=places.cities\n",
    "        if not countries:\n",
    "            if not cities:\n",
    "                location='loc'\n",
    "                #print(sentence)\n",
    "            else:\n",
    "                location=cities[0]\n",
    "                print(\"city found: \",location)\n",
    "        else:\n",
    "            location=countries[0]\n",
    "            print(\"country found: \",location)\n",
    "        if \"?\" in location:\n",
    "            location='loc'\n",
    "    return location\n",
    "possible_locations = pd.DataFrame(columns = ['location']) \n",
    "possible_locations[\"location\"]=data_train['text'].apply(fill_location)\n",
    "data_train[\"location\"].fillna(possible_locations[\"location\"], inplace=True)#fill nan\n",
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K49PYOYYbk3J"
   },
   "outputs": [],
   "source": [
    "#replace all nan in keyword feature\n",
    "keyword_list=data_train[\"keyword\"].unique() #list all kewords in keyword feature\n",
    "keyword_list = np.append(keyword_list, ['bomber','detonted','explosives','fires',\n",
    "                                        'calamity','cataclysm','act of God','holocaust',\n",
    "                                       'mishap','heat wave'])\n",
    "def fill_keyword(sentence):\n",
    "    words=sentence.split()\n",
    "    keyword = [ key for key in keyword_list if key in words ] #check if any keword from keyword feature is present in tweet\n",
    "    try:\n",
    "        keyword=keyword[0]\n",
    "    except:\n",
    "        keyword='key'\n",
    "    return keyword\n",
    "possible_keywords = pd.DataFrame(columns = ['keyword']) \n",
    "possible_keywords[\"keyword\"]=data_train['text'].apply(fill_keyword) \n",
    "data_train[\"keyword\"].fillna(possible_keywords[\"keyword\"], inplace=True)#fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTnYcsr1bk3J"
   },
   "outputs": [],
   "source": [
    "loc_dict={'United States':'USA','New York':'USA',\"London\":'UK',\"Los Angeles, CA\":'USA',\"Washington, D.C.\":'USA',\n",
    "          \"California\":'USA',\"Chicago, IL\":'USA',\"Chicago\":'USA',\"New York, NY\":'USA',\"California, USA\":'USA',\n",
    "          \"FLorida\":'USA',\"Everywhere\":'Worldwide',\"San Francisco\":'USA',\"Florida\":'USA',\"United Kingdom\":'UK',\n",
    "          \"Los Angeles\":'USA',\"Toronto\":'Canada',\"San Francisco, CA\":'USA',\"NYC\":'USA',\"Seattle\":'USA',\n",
    "          \"Earth\":'Worldwide',\"Ireland\":'UK',\"London, England\":'UK',\"New York City\":'USA',\"Texas\":'USA',\n",
    "          \"London, UK\":'UK',\"Atlanta, GA\":'USA',\"Mumbai\":\"India\",\"US\":\"USA\",\"us\":\"USA\",\"U.S.\":\"USA\",\"IN\":\"India\"}\n",
    "\n",
    "data_train['location'].replace(loc_dict,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "oix9j1Ykbk3K",
    "outputId": "4b97f320-8f91-4303-c311-0c20e3aaf86e"
   },
   "outputs": [],
   "source": [
    "sns.barplot(y=data_train['location'].value_counts()[:10].index,\n",
    "            x=data_train['location'].value_counts()[:10],\n",
    "            orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-V9IlYsbk3K"
   },
   "outputs": [],
   "source": [
    "key_dict={'panic':'fear','suicide bomber':'bomb','suicide bomb':'bomb',\n",
    "          'suicide bombing':'bomb',\"mass murder\":\"massacre\",\"hurricane\":\"typhoon\"}\n",
    "data_train['keyword'].replace(key_dict,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhHlNWflbk3L"
   },
   "outputs": [],
   "source": [
    "def clean_str(txt):\n",
    "    #replace and simplify a lot of signals\n",
    "    txt = txt.lower()#turn to lower case\n",
    "    txt = re.sub(r'http\\S+', 'link', txt)\n",
    "    txt = txt.replace(\",000,000\", \"m\").replace(\",000\",\"k\").replace(\"′\", \"'\").replace(\"’\", \"'\").replace(\"can't\", \"cannot\")\\\n",
    "             .replace(\"don't\", \"do not\").replace(\"isn't\", \"is not\").replace(\"you're\", \"you are\").replace(\"you've\",\"you have\")\\\n",
    "             .replace(\"you'll\", \"you will\").replace(\"that'll\",'that will').replace(\"should've\", \"should have\")\\\n",
    "             .replace(\"aren't\", \"are not\").replace(\"couldn't\", \"could not\").replace(\"didn't\", \"did not\")\\\n",
    "             .replace(\"doesn't\", \"does not\").replace(\"hadn't\", \"had not\").replace(\"hasn't\", \"has not\")\\\n",
    "             .replace(\"haven't\", \"have not\").replace(\"mustn't\", \"must not\").replace(\"wasn't\", \"was not\")\\\n",
    "             .replace(\"i'm\", \"i am\").replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"ain't\", \"am not\")\\\n",
    "             .replace(\"%\", \" \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \").replace(\"€\", \" euro \")\\\n",
    "             .replace(\"'ll\", \" will\").replace(\"'ve\", \" have\").replace(\"it's\", \"it is\")\n",
    "    \n",
    "    txt = re.sub('[^A-Za-z]+', ' ', txt) #removing html tags\n",
    "    txt = re.sub('[?|!|\\'|\"|#]','',txt) #removing punctuations\n",
    "    txt = re.sub('[.|,|)|(|\\|/]','',txt) #removing punctuations\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ymoo4YREbk3L",
    "outputId": "decb7add-aba2-4a19-a473-5e5d2cd936e1"
   },
   "outputs": [],
   "source": [
    "cleaned_str=clean_str(\"Horrible Accident | Man Died In Wings of AirplaneåÊ(29-07-2015) http://t.co/wq3wJsgPHL\")\n",
    "data_train['text'] = data_train['text'].apply(clean_str)\n",
    "data_train['keyword'] = data_train['keyword'].apply(clean_str)\n",
    "data_train['location'] = data_train['location'].apply(clean_str)\n",
    "cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "9tt-ZDdNbk3M",
    "outputId": "12567100-f162-45c5-c661-e4347257db51"
   },
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNMS_746bk3M",
    "outputId": "50baed57-5d69-4541-d0cc-633713120890"
   },
   "outputs": [],
   "source": [
    "data_train[data_train['location']==\" \"] ['location'].replace('loc',inplace=True)\n",
    "data_train[data_train['location']==\" \"] ['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqehN5Svbk3N"
   },
   "outputs": [],
   "source": [
    "data_raw_non_Disaster = data_train[data_train['target'] == 0]\n",
    "data_raw_Disaster = data_train[data_train['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "A5iHs4Iybk3N",
    "outputId": "3e9b4bc2-2c2d-45da-c408-a392defae27e"
   },
   "outputs": [],
   "source": [
    "sns.barplot(y=data_raw_non_Disaster['keyword'].value_counts()[:10].index,\n",
    "            x=data_raw_non_Disaster['keyword'].value_counts()[:10],\n",
    "            orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "HkK0qkmFbk3N",
    "outputId": "bcda3582-995d-4570-cd1b-b29c5a9e81d2"
   },
   "outputs": [],
   "source": [
    "sns.barplot(y=data_raw_Disaster['keyword'].value_counts()[:10].index,\n",
    "            x=data_raw_Disaster['keyword'].value_counts()[:10],\n",
    "            orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "zsBKha6Hbk3O",
    "outputId": "140d92c2-47ca-4b04-8df3-1c05c0c867b7"
   },
   "outputs": [],
   "source": [
    "words = '' \n",
    "for val in data_raw_non_Disaster[\"keyword\"]:\n",
    "     val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "     tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "     for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower()\n",
    "        words += \" \".join(tokens)+\" \"\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate(words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "G5jUrYhHbk3O",
    "outputId": "a1514c65-53ef-440b-f3dd-9f799b172e7d"
   },
   "outputs": [],
   "source": [
    "words = '' \n",
    "for val in data_raw_Disaster[\"keyword\"]:\n",
    "     val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "     tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "     for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower()\n",
    "        words += \" \".join(tokens)+\" \"\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate(words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "UzFlqUmybk3P",
    "outputId": "64a9c0be-2d9c-46c1-abc5-01e36ee24a0e"
   },
   "outputs": [],
   "source": [
    "words = '' \n",
    "for val in data_raw_non_Disaster[\"location\"]:\n",
    "     val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "     tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "     for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower()\n",
    "        words += \" \".join(tokens)+\" \"\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate(words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "FP6H7PZ9bk3P",
    "outputId": "fa8902bf-a723-4a31-f1aa-dcd74955717c"
   },
   "outputs": [],
   "source": [
    "words = '' \n",
    "for val in data_raw_Disaster[\"location\"]:\n",
    "     val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "     tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "     for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower()\n",
    "        words += \" \".join(tokens)+\" \"\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate(words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCUGP51Ubk3Q",
    "outputId": "26432a93-13bc-4e5f-cf19-1047dc08df4c"
   },
   "outputs": [],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6x0sGUUbk3Q",
    "outputId": "c897accb-a886-4412-df80-b192125bad92"
   },
   "outputs": [],
   "source": [
    "#balance data with equal no of disaster and non disaster tweets\n",
    "data_raw_non_Disaster = data_train[data_train['target'] == 0]\n",
    "data_raw_Disaster = data_train[data_train['target'] == 1]\n",
    "diff=len(data_raw_non_Disaster)-len(data_raw_Disaster)    \n",
    "data_sample=data_raw_Disaster.sample(round(diff/2),axis=0)\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWBHjCjVbk3R",
    "outputId": "48dd400f-8000-4cc9-c9ea-f2681de7923b"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "temp_df = pd.DataFrame ( columns = data_train.columns)\n",
    "\n",
    "#sequences = []\n",
    "#targets = []\n",
    "\n",
    "for index, row in data_sample.iterrows():\n",
    "    seqs = []\n",
    "    text = row['text']\n",
    "    id=row['id']\n",
    "    keyword=row['keyword']\n",
    "    location=row['location']\n",
    "    target=row['target']\n",
    "\n",
    "    # if empty text, skipping to next row data\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "   # seqs.append(text)\n",
    "    #temp_df.loc[len(temp_df.index)] = [id, keyword, location,text,target] \n",
    "    # apply data augmentation\n",
    "    \n",
    "    # random deletion\n",
    "    seq2 = t.random_deletion(text, p=0.2)\n",
    "    if type(seq2) == type([]):\n",
    "        #seqs.append(seq2[0])\n",
    "        temp_df.loc[len(temp_df.index)] = [id, keyword, location,seq2[0],target]\n",
    "    else:\n",
    "        #seqs.append(seq2)\n",
    "        temp_df.loc[len(temp_df.index)] = [id, keyword, location,seq2,target]\n",
    "        # random swap\n",
    "    #if len(text) > 1:\n",
    "      #  seq3=t.random_swap(text)\n",
    "        #seqs.append(seq3)\n",
    "      #  temp_df.loc[len(temp_df.index)] = [id, keyword, location,seq3,target]\n",
    "    # synonym replacement and random insertion\n",
    "    for i in range(2):\n",
    "        seq4=t.synonym_replacement(text)\n",
    "        #seqs.append(seq4) \n",
    "        temp_df.loc[len(temp_df.index)] = [id, keyword, location,seq4,target]\n",
    "       # try:\n",
    "         #   seq5=t.random_insertion(text)\n",
    "         #   temp_df.loc[len(temp_df.index)] = [id, keyword, location,seq5,target]\n",
    "            #seqs.append(seq5)\n",
    "       # except:\n",
    "         #   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59W_M9HMbk3S",
    "outputId": "ec32a7cd-7ea4-49e5-d469-363c640783f8"
   },
   "outputs": [],
   "source": [
    "data_train=pd.concat([data_train, temp_df],ignore_index=True)\n",
    "\n",
    "data_raw_non_Disaster = data_train[data_train['target'] == 0]\n",
    "data_raw_Disaster = data_train[data_train['target'] == 1]\n",
    "print(\"Non Disatser Shape = \"+str(data_raw_non_Disaster.shape))\n",
    "print(\"Disaster Shape = \"+str(data_raw_Disaster.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rdzfXCbubk3S",
    "outputId": "c0ccbed2-b30d-4a3b-a698-48a1e59cf767"
   },
   "outputs": [],
   "source": [
    "length=len(data_raw_non_Disaster.index)\n",
    "totalFrame = [data_raw_non_Disaster, data_raw_Disaster[:length]] #balanced dataframe\n",
    "data_new = pd.concat(totalFrame)\n",
    "data_new.sort_values('id', axis=0, ascending=True, inplace=True)\n",
    "data_new.reset_index(drop = True, inplace = True)\n",
    "data_new.to_csv(os.path.join(DATAPATH, TEMP_DF), index = False)\n",
    "data_train = pd.read_csv(os.path.join(DATAPATH, TEMP_DF))\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "LfHcRCyPbk3T",
    "outputId": "fcddd2d1-6c7e-4ffe-b4f0-7081673c170e"
   },
   "outputs": [],
   "source": [
    "sns.countplot(data_train.target) #plot disaster and non disatser tweets\n",
    "plt.xlabel('Target')\n",
    "plt.title('Number of disaster tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPX78SoMbk3U"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))  #stopwords for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def remove_stopwords(txt):\n",
    "    word_list = word_tokenize(txt)\n",
    "    no_stop_words = ' '.join(str(stemmer.stem(w)) for w in word_list if w not in stop_words and len(w)!=1)\n",
    "    return no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h81mC2ibk3U"
   },
   "outputs": [],
   "source": [
    "data_train['text'] = data_train['text'].apply(remove_stopwords)  #remove stopwords from tweet\n",
    "def stem_words(w):\n",
    "    return stemmer.stem(w)\n",
    "data_train['keyword'] = data_train['keyword'].apply(stem_words)\n",
    "data_train['location'] = data_train['location'].apply(stem_words)\n",
    "data_train['text']=data_train['location']+\" \"+data_train['keyword']+\" \"+data_train['text']  #combining location, keword and text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "5K_4pseHbk3V",
    "outputId": "b2abc817-374e-4a77-8d3e-5feffce79cd7"
   },
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "4HqctWLbbk3V",
    "outputId": "7b0af0c5-cb02-4284-d42d-664d9c1f11e2"
   },
   "outputs": [],
   "source": [
    "data_raw_non_Disaster = data_train[data_train['target'] == 0]\n",
    "data_raw_Disaster = data_train[data_train['target'] == 1]\n",
    "sns.barplot(y=data_raw_non_Disaster['keyword'].value_counts()[:10].index, #plot non disaster kewords\n",
    "            x=data_raw_non_Disaster['keyword'].value_counts()[:10],\n",
    "            orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "JKgKJEJ8bk3W",
    "outputId": "17e7c6db-7feb-4db0-e177-a634affa3094"
   },
   "outputs": [],
   "source": [
    "sns.barplot(y=data_raw_Disaster['keyword'].value_counts()[:10].index,  #plot disaster kewords\n",
    "            x=data_raw_Disaster['keyword'].value_counts()[:10],\n",
    "            orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pqK-adHbk3W"
   },
   "outputs": [],
   "source": [
    "X=data_train[['text']]\n",
    "Y=data_train['target']\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.2, \n",
    "                                                    random_state=42) #split data for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45ddaXm4bk3W"
   },
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "Text_tokenizer = Tokenizer(num_words=max_words)  \n",
    "# Updates internal vocabulary based on a list of texts. \n",
    "# This method creates the vocabulary index based on word frequency. \n",
    "\n",
    "train_text=X_train['text'].tolist()\n",
    "validation_text=X_validation['text'].tolist()\n",
    "#test_text=X_test['text'].tolist()\n",
    "\n",
    "Text_tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# Transforms each row from texts to a sequence of integers. \n",
    "# So it basically takes each word in the text and replaces it \n",
    "train_text = Text_tokenizer.texts_to_sequences(train_text)\n",
    "validation_text = Text_tokenizer.texts_to_sequences(validation_text)\n",
    "#test_text = Text_tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "\n",
    "# Pad sequences\n",
    "train_text = pad_sequences(train_text, maxlen=40, dtype='int32', \n",
    "                                           padding='pre', truncating='pre', value=0.)\n",
    "validation_text = pad_sequences(validation_text, maxlen=40, dtype='int32', \n",
    "                                           padding='pre', truncating='pre', value=0.)\n",
    "#test_text = pad_sequences(test_text, maxlen=40, dtype='int32', \n",
    "                                           #padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "text_word_index = Text_tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REnP1SWibk3X"
   },
   "outputs": [],
   "source": [
    "#convert dataframe to array\n",
    "TrainFinal_RNN=np.array(train_text)\n",
    "CVFinal_RNN=np.array(validation_text)\n",
    "#TestFinal_RNN=np.array(TestFinal_RNN)\n",
    "\n",
    "TrainFinalLabels = Y_train\n",
    "CVFinalLabels = Y_validation\n",
    "#TestFinalLabels = Y_test\n",
    "\n",
    "TrainFinalLabels_RNN=np.array(TrainFinalLabels)\n",
    "CVFinalLabels_RNN=np.array(CVFinalLabels)\n",
    "#TestFinalLabels_RNN=np.array(TestFinalLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m_KD9-obk3X"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VErdiokrbk3X"
   },
   "outputs": [],
   "source": [
    "#LSTM model\n",
    "l2_reg = l2(0.001)\n",
    "max_sequence_length = 40\n",
    "model_file = 'model.h5'\n",
    "num_classes = 2\n",
    "embedding_size = 40\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(max_words, embedding_size, input_length=max_sequence_length, embeddings_regularizer=l2_reg))\n",
    "    \n",
    "    model.add(SpatialDropout1D(0.5))\n",
    "    \n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2_reg, \n",
    "                   recurrent_regularizer=l2_reg, bias_regularizer=l2_reg))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    optimizer=Adam(learning_rate=0.0001, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KmgOI_zbk3X",
    "outputId": "4b9519e8-9021-4957-dec8-f5e1f8a1a232"
   },
   "outputs": [],
   "source": [
    "#training using LSTM\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=5)\n",
    "model = model_fn()\n",
    "history = model.fit(TrainFinal_RNN, TrainFinalLabels_RNN,\n",
    "          validation_data=(CVFinal_RNN, CVFinalLabels_RNN),\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          verbose=1,\n",
    "          callbacks=[earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "E_3buLoMbk3Y",
    "outputId": "53a2904d-327a-4e86-cd00-61798d2903b3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show() #plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "llZbyb22bk3Y",
    "outputId": "0a3a8a1a-68d7-4cc5-c3ab-162b852cf8d7"
   },
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()#plot accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4Yh2HJFbk3Y",
    "outputId": "a071c6ac-aa03-4cc6-bf14-fdd1b041e329"
   },
   "outputs": [],
   "source": [
    "data_predict = pd.read_csv(\"/content/train.csv\")\n",
    "data_predict.info()#test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJIC6_Bwbk3Z",
    "outputId": "0e95badd-0177-42b4-9d7a-f90a945569b8"
   },
   "outputs": [],
   "source": [
    "#process test data\n",
    "\n",
    "possible__predict_keywords = pd.DataFrame(columns = ['keyword']) \n",
    "possible__predict_keywords[\"keyword\"]=data_predict['text'].apply(fill_keyword)\n",
    "data_predict[\"keyword\"].fillna(possible__predict_keywords[\"keyword\"], inplace=True)\n",
    "data_predict['keyword'].replace(key_dict,inplace=True)\n",
    "\n",
    "possible__predict_locations = pd.DataFrame(columns = ['location']) \n",
    "possible__predict_locations[\"location\"]=data_predict['text'].apply(fill_location)\n",
    "data_predict[\"location\"].fillna(possible_locations[\"location\"], inplace=True)\n",
    "data_predict['location'].replace(loc_dict,inplace=True)\n",
    "\n",
    "data_predict['text'] = data_predict['text'].apply(clean_str)\n",
    "data_predict['keyword'] = data_predict['keyword'].apply(clean_str)\n",
    "data_predict['location'] = data_predict['location'].apply(clean_str)\n",
    "\n",
    "data_predict['text'] = data_predict['text'].apply(remove_stopwords)\n",
    "data_predict['keyword'] = data_predict['keyword'].apply(stem_words)\n",
    "data_predict['location'] = data_predict['location'].apply(stem_words)\n",
    "\n",
    "data_predict['text']=data_predict['location']+\" \"+data_predict['keyword'] +\" \"+data_predict['text']  \n",
    "\n",
    "predict_text=data_predict['text'].tolist()\n",
    "predict_text = Text_tokenizer.texts_to_sequences(predict_text)\n",
    "predict_text = pad_sequences(predict_text, maxlen=40, dtype='int32', \n",
    "                                           padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "PredictFinal_RNN=np.array(predict_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLRawh__bk3Z",
    "outputId": "d5d58a77-486b-4d19-9fc1-e96c99b0a8b9"
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(PredictFinal_RNN) #prediction\n",
    "\n",
    "binary_predicted = np.array(predicted) >= 0.5\n",
    "targets = binary_predicted.astype(int).reshape((len(binary_predicted)))\n",
    "my_submission = pd.DataFrame({'id': data_predict.id, 'target': targets})\n",
    "my_submission.to_csv(os.path.join(DATAPATH, 'predict_lstm.csv'), index=False)\n",
    "\n",
    "print(\"Submission file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4mTx7U1bk3Z"
   },
   "outputs": [],
   "source": [
    "def listOfListSent(TrainText):\n",
    "    listOfSentences = []\n",
    "    for sentence in TrainText.values:\n",
    "        subSentence = []\n",
    "        for word in sentence.split():\n",
    "            subSentence.append(word)  \n",
    "        listOfSentences.append(subSentence)\n",
    "    return listOfSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SCi3yFBbk3a"
   },
   "outputs": [],
   "source": [
    "# first create w2v feature only for question1 in the order train of question1 then cv of question1 then test of question1\n",
    "# second create w2v feature only for question2 in the order train of question2 then cv of question2 then test of question2\n",
    "\n",
    "def create_w2v(df, which_df, which_ques):\n",
    "    listOfSentences = listOfListSent(df[which_ques])\n",
    "    if which_df == \"Train\":\n",
    "        # compute average word2vec for each text in train data.\n",
    "        w2vModel = gensim.models.Word2Vec(listOfSentences, size=300, min_count=5, workers=4)\n",
    "        pickle.dump(w2vModel, open(os.path.join(DATAPATH, \"w2vModel.sav\"), 'wb'))\n",
    "        train_text_feature_w2v = []\n",
    "        for sentence in listOfSentences:\n",
    "            sentenceVector = np.zeros(300)\n",
    "            TotalWordsPerSentence = 0\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    vect = w2vModel.wv[word]\n",
    "                    sentenceVector += vect\n",
    "                    TotalWordsPerSentence += 1\n",
    "                except:\n",
    "                    pass\n",
    "            if TotalWordsPerSentence!= 0:\n",
    "                sentenceVector /= TotalWordsPerSentence\n",
    "            train_text_feature_w2v.append(sentenceVector)\n",
    "        return train_text_feature_w2v\n",
    "    else:\n",
    "        # compute average word2vec for each text in cv and test data.\n",
    "        w2vModelLoaded = pickle.load(open(os.path.join(DATAPATH, \"w2vModel.sav\"), 'rb'))\n",
    "        cv_test_text_feature_w2v = []\n",
    "        for sentence in listOfSentences:\n",
    "            sentenceVector = np.zeros(300)\n",
    "            TotalWordsPerSentence = 0\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    vect = w2vModelLoaded.wv[word]\n",
    "                    sentenceVector += vect\n",
    "                    TotalWordsPerSentence += 1\n",
    "                except:\n",
    "                    pass\n",
    "            if TotalWordsPerSentence!= 0:\n",
    "                sentenceVector /= TotalWordsPerSentence\n",
    "            cv_test_text_feature_w2v.append(sentenceVector)\n",
    "        return cv_test_text_feature_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN0Xf1Hzbk3a"
   },
   "outputs": [],
   "source": [
    "w2vFeaturesTrainText = create_w2v(X_train, \"Train\", \"text\")\n",
    "w2vFeaturesCVText = create_w2v(X_validation, \"CV\", \"text\")\n",
    "#w2vFeaturesTestText = create_w2v(X_test, \"Test\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eR6gMxRrbk3a",
    "outputId": "5475ebba-774f-43f0-82c4-fcbca09fda32"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of w2vFeaturesTrainText = {}\".format(np.array(w2vFeaturesTrainText).shape))\n",
    "print(\"Shape of w2vFeaturesCVText = {}\".format(np.array(w2vFeaturesCVText).shape))\n",
    "#print(\"Shape of w2vFeaturesTestText = {}\\n\".format(np.array(w2vFeaturesTestText).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsX6u0BBbk3a",
    "outputId": "e38eb883-f663-459a-dbd2-18062ecb66c8"
   },
   "outputs": [],
   "source": [
    "TrainFinal = np.array(w2vFeaturesTrainText)#final train data\n",
    "TrainFinalLabels = Y_train\n",
    "print(\"Shape of Train Data = {}\".format(TrainFinal.shape))\n",
    "print(\"Shape of Train Labels = {}\\n\".format(TrainFinalLabels.shape))\n",
    "pickle.dump(TrainFinal, open(os.path.join(DATAPATH, 'TrainFinal.sav'), 'wb'))\n",
    "pickle.dump(TrainFinalLabels, open(os.path.join(DATAPATH, 'TrainFinalLabels.sav'), 'wb'))\n",
    "\n",
    "CVFinal = (np.array(w2vFeaturesCVText))\n",
    "CVFinalLabels = Y_validation\n",
    "print(\"Shape of CV Data = {}\".format(CVFinal.shape))\n",
    "print(\"Shape of CV Labels = {}\\n\".format(CVFinalLabels.shape))\n",
    "pickle.dump(CVFinal, open(os.path.join(DATAPATH, 'CVFinal.sav'), 'wb'))\n",
    "pickle.dump(CVFinalLabels, open(os.path.join(DATAPATH, 'CVFinalLabels.sav'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RWQtKJubk3b"
   },
   "outputs": [],
   "source": [
    "#method for printing confusion matrix\n",
    "def print_confusionMatrix(Y_TestLabels, PredictedLabels):\n",
    "    confusionMatx = confusion_matrix(Y_TestLabels, PredictedLabels)\n",
    "    \n",
    "    precision = confusionMatx/confusionMatx.sum(axis = 0)\n",
    "    \n",
    "    recall = (confusionMatx.T/confusionMatx.sum(axis = 1)).T\n",
    "    \n",
    "    # confusionMatx = [[1, 2],\n",
    "    #                  [3, 4]]\n",
    "    # confusionMatx.T = [[1, 3],\n",
    "    #                   [2, 4]]\n",
    "    # confusionMatx.sum(axis = 1)  axis=0 corresponds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # confusionMatx.sum(axix =1) = [[3, 7]]\n",
    "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)) = [[1/3, 3/7]\n",
    "    #                                                  [2/3, 4/7]]\n",
    "\n",
    "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)).T = [[1/3, 2/3]\n",
    "    #                                                    [3/7, 4/7]]\n",
    "    # sum of row elements = 1\n",
    "    plt.figure(figsize=(25, 7))\n",
    "    labels = [0, 1]\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(confusionMatx, cmap = \"Blues\", annot = True, fmt = \".1f\", xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 18})\n",
    "    plt.title(\"Confusion Matrix\", fontsize = 20)\n",
    "    plt.xlabel('Predicted Class', fontsize = 18)\n",
    "    plt.ylabel('Original Class', fontsize = 18)\n",
    "    plt.tick_params(labelsize = 18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(precision, cmap = \"Blues\", annot = True, fmt = \".2f\", xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 18})\n",
    "    plt.title(\"Precision Matrix\", fontsize = 20)\n",
    "    plt.xlabel('Predicted Class', fontsize = 18)\n",
    "    plt.ylabel('Original Class', fontsize = 18)\n",
    "    plt.tick_params(labelsize = 18)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.heatmap(recall, cmap = \"Blues\", annot = True, fmt = \".2f\", xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 18})\n",
    "    plt.title(\"Recall Matrix\", fontsize = 20)\n",
    "    plt.xlabel('Predicted Class', fontsize = 18)\n",
    "    plt.ylabel('Original Class', fontsize = 18)\n",
    "    plt.tick_params(labelsize = 18)\n",
    "    \n",
    "    plt.subplots_adjust(wspace = 0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "FespI7ovbk3b",
    "outputId": "ef5bcf5b-5286-4e62-e827-1678dc94eb61"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "Scalar = StandardScaler()\n",
    "Scalar.fit(TrainFinal)\n",
    "TrainFinalStd = Scalar.transform(TrainFinal)\n",
    "CVFinalStd = Scalar.transform(CVFinal)\n",
    "\n",
    "alpha = [10**x for x in range(-5, 3)]\n",
    "\n",
    "cv_log_loss = []\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(loss = \"log\", alpha = i)\n",
    "    clf.fit(TrainFinalStd, TrainFinalLabels)\n",
    "    calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "    calib_clf.fit(TrainFinalStd, TrainFinalLabels)\n",
    "    predicted_y = calib_clf.predict_proba(CVFinalStd)\n",
    "    cv_log_loss.append(log_loss(CVFinalLabels, predicted_y))\n",
    "    print(\"For alpha value of \"+str(i)+\" CV log loss = \"+str(log_loss(CVFinalLabels, predicted_y))) #finding the best alpha for Stochastic Gradient Descent \n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "plt.xscale('log')\n",
    "plt.plot(alpha, cv_log_loss)\n",
    "for xy in zip(alpha, np.round(cv_log_loss, 4)):\n",
    "    plt.annotate(xy, xy)\n",
    "    \n",
    "plt.title(\"Alpha vs Log-loss\", fontsize = 20)\n",
    "plt.xlabel(\"Alpha\", fontsize = 20)\n",
    "plt.ylabel(\"Log-Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "yln8e1CRbk3b",
    "outputId": "dc62b839-fd45-4a2c-be66-113c1e79956a"
   },
   "outputs": [],
   "source": [
    "#train sgd model and see confusion matrix\n",
    "import seaborn as sns\n",
    "Log_Loss = []\n",
    "Accuracy_Scores = []\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "best_alpha = alpha[np.argmin(cv_log_loss)]\n",
    "clf = SGDClassifier(loss = \"log\", alpha = best_alpha)\n",
    "clf.fit(TrainFinalStd, TrainFinalLabels)\n",
    "calib_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\n",
    "calib_clf.fit(TrainFinalStd, TrainFinalLabels)\n",
    "\n",
    "predict_test = calib_clf.predict_proba(CVFinalStd)\n",
    "LR_LogLoss = log_loss(CVFinalLabels, predict_test)\n",
    "LR_Accuracy = accuracy_score(CVFinalLabels, calib_clf.predict(CVFinalStd))\n",
    "print('For values of best alpha = {}, Test LogLoss = {}, Test Accuracy Score = {}%'.format(best_alpha, LR_LogLoss, round((float(LR_Accuracy))*100,2)))\n",
    "print_confusionMatrix(CVFinalLabels, calib_clf.predict(CVFinalStd))\n",
    "\n",
    "Log_Loss.append(LR_LogLoss)\n",
    "Accuracy_Scores.append(LR_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0_0-amBbk3c",
    "outputId": "f23d5938-5beb-4bd9-b42f-83d08474ace7"
   },
   "outputs": [],
   "source": [
    "#xgboost\n",
    "import xgboost as xgb\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(TrainFinal, label=TrainFinalLabels)\n",
    "d_test = xgb.DMatrix(CVFinal, label=CVFinalLabels)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'valid')]\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "agPOdU_tbk3c",
    "outputId": "1db68052-3e66-4958-b084-f62fbddcbdf4"
   },
   "outputs": [],
   "source": [
    "#predict and see confusion matrix (xgboost)\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "Log_Loss = []\n",
    "Accuracy_Scores = []\n",
    "d_test = xgb.DMatrix(CVFinal)\n",
    "LR_LogLoss = log_loss(CVFinalLabels, bst.predict(d_test))\n",
    "LR_Accuracy = accuracy_score(CVFinalLabels, list(map(lambda x: int(x>0.5), bst.predict(d_test))))\n",
    "print('Test LogLoss = {}, Test Accuracy Score = {}%'.format(LR_LogLoss, round((float(LR_Accuracy))*100,2)))\n",
    "print_confusionMatrix(CVFinalLabels,  list(map(lambda x: int(x>0.5), bst.predict(d_test))))\n",
    "\n",
    "Log_Loss.append(LR_LogLoss)\n",
    "Accuracy_Scores.append(LR_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDzzTWZobk3c",
    "outputId": "6958d6b9-1464-49d8-aaf3-aafbfce90001"
   },
   "outputs": [],
   "source": [
    "w2vFeaturesPredictText = create_w2v(data_predict, \"Test\", \"text\")# process test data\n",
    "\n",
    "PredictFinal = (np.array(w2vFeaturesPredictText))\n",
    "print(\"Shape of Predict Data = {}\".format(PredictFinal.shape))\n",
    "pickle.dump(PredictFinal, open(os.path.join(DATAPATH, 'PredictFinal.sav'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lc57lyMKbk3c"
   },
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(PredictFinal) #predict for test data\n",
    "result= pd.DataFrame({'id':data_predict['id'],\n",
    "                      'target':list(map(lambda x: int(x>0.5), bst.predict(d_test)))})\n",
    "result.to_csv(os.path.join(DATAPATH, 'predict.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_with_Disaster_Tweets.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
